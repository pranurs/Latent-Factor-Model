{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Factor Model - Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    '''\n",
    "        Constructs ratings matrix from the Movie Lens Database file having information on users and their ratings.\n",
    "        \n",
    "        Then, extracts all non-zero data entries (userID, movieID, rating) from the sparse matrix, which then forms\n",
    "        the dataset for use in the Latent Factor Model.\n",
    "        \n",
    "        Further, splits the dataset of known ratings into train, test and validation datasets in the ratio 0.75:0.15:0.1.\n",
    "        \n",
    "        Returns: \n",
    "            1) Constructed sparse ratings matrix\n",
    "            2) Samples for training\n",
    "            3) Samples for testing\n",
    "            4) Samples for validation\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # ratingsInfo = pd.read_csv('ratingsActual.csv', usecols = [0,1,2], header = None)  \n",
    "    \n",
    "    # print(len(ratingsInfo))    # 1000209\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Constructing Ratings Matrix ###########\n",
    "    \n",
    "    \n",
    "    # ratingsMatrix = np.zeros((6040,3952))\n",
    "    # ratingsCount = 0\n",
    "    # for i in range(len(ratingsInfo)):\n",
    "    #    ratingsMatrix[ratingsInfo.iloc[i,0]-1][ratingsInfo.iloc[i,1]-1] = ratingsInfo.iloc[i,2]\n",
    "    #    ratingsCount += 1\n",
    "        \n",
    "    # np.save(\"ratingsMatrix.npy\",ratingsMatrix)    \n",
    "    \n",
    "    ratingsMatrix = np.load(\"ratingsMatrix.npy\")\n",
    "    totalUsers = ratingsMatrix.shape[0]\n",
    "    totalMovies = ratingsMatrix.shape[1]\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Getting all non-zero samples from Ratings Matrix for use in the Latent Factor Model ###########\n",
    "    \n",
    "    \n",
    "    totalSamples = [(user,movie,ratingsMatrix[user][movie]) for user in range(totalUsers) for movie in range(totalMovies) if ratingsMatrix[user][movie]>0] \n",
    "    \n",
    "    ratingsCount = len(totalSamples)\n",
    "    \n",
    "    # print(len(totalSamples))    # 1000209\n",
    "\n",
    "    np.random.shuffle(totalSamples)    \n",
    "    \n",
    "    ########################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Splitting Known Ratings into Train, Test and Validation Sets:   (In ratio *0.75 : 0.15 : 0.10* respectively) ########\n",
    "    \n",
    "\n",
    "    trainSamples = totalSamples[ : int(0.75 * ratingsCount)]\n",
    "    testSamples = totalSamples[int(0.75 * ratingsCount) : int(0.90 * ratingsCount)]\n",
    "    valSamples = totalSamples[int(0.90 * ratingsCount) : ]\n",
    "    \n",
    "    ###########################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ratingsMatrix, trainSamples, testSamples, valSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the matrices P, Q, and biases b, b_u, b_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights (trainSamples, K, totalUsers, totalMovies):\n",
    "    '''\n",
    "        Initializes the user-feature and item-feature matrices with values from a normal distribution,\n",
    "        scaled by 1/K.\n",
    "        The global bias b is the mean of all the known ratings in the matrix.\n",
    "        The user and item biases are initialized to zeros.\n",
    "    '''\n",
    "\n",
    "    # print(totalUsers, totalMovies)\n",
    "    \n",
    "    P = np.random.normal(scale = 1/K, size = (totalUsers, K))\n",
    "    Q = np.random.normal(scale = 1/K, size = (totalMovies, K))\n",
    "\n",
    "    b_u = np.zeros(totalUsers)\n",
    "    b_m = np.zeros(totalMovies)\n",
    "    \n",
    "    ratings = np.array([rating for (user,movie,rating) in trainSamples])\n",
    "    b = np.mean(ratings)\n",
    "    \n",
    "    # print(b) ---- > b = 3.581564453029317 for full rating matrix\n",
    "    \n",
    "    return P, Q, b_u, b_m, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating (i, j, b_u_i, b_m_j, b, P_i, Q_j):\n",
    "    '''\n",
    "        Returns a single value for the predicted rating for a user i and item j.\n",
    "    '''\n",
    "    \n",
    "    rating = b + b_u_i + b_m_j + np.dot(P_i, Q_j.T)\n",
    "    \n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_ratings (P, Q, b_u, b_m, b):\n",
    "    '''\n",
    "        Based on the optimized user-feature matrix, item-feature matrix and biases (user, item, global),\n",
    "        this function returns a full user-item matrix containing the predicted values for each user-item pair\n",
    "        in the original ratings matrix.\n",
    "    '''\n",
    "    \n",
    "    allRatings = b + b_u[:,np.newaxis] + b_m[np.newaxis,:] + np.matmul(P,Q.T)   \n",
    "    allRatings [allRatings > 5] = 5\n",
    "    allRatings [allRatings < 1] = 1\n",
    "    \n",
    "    return allRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors (predictedRatingMatrix, actualRatings) :\n",
    "    '''\n",
    "        Given the predicted rating matrix, and actual ratings, this function returns the following\n",
    "        errors:\n",
    "        1) Sum of Squares Error (SSE)\n",
    "        2) Mean Square Error (MSE)\n",
    "        3) Root Mean Square Error (RMSE)\n",
    "        4) Mean Absolute Error (MAE)\n",
    "    '''\n",
    "    \n",
    "    SSE = 0\n",
    "    SAE = 0\n",
    "    \n",
    "    for user, movie, trueRating in actualRatings :\n",
    "        \n",
    "        SSE += (trueRating - predictedRatingMatrix[user,movie]) ** 2\n",
    "        SAE += abs(trueRating - predictedRatingMatrix[user,movie])\n",
    "    \n",
    "    MSE = SSE / len(actualRatings)\n",
    "    RMSE = MSE ** 0.5\n",
    "    MAE = SAE / len(actualRatings)\n",
    "    \n",
    "    return SSE, MSE, RMSE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent - Using Alternate Least Squares Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_ALS (alpha, epochs, trainSamples, K, ratingsMatrix, lambda_reg = 0):\n",
    "    '''\n",
    "    \n",
    "        Performs stochastic gradient descent on the training samples, and returns the optimized user-feature\n",
    "        and item-feature matrices P and Q respectively, along with the optimized biases for users and items, \n",
    "        and the global bias.\n",
    "        \n",
    "        This variation uses the alternate least squares (ALS) method in order to guarantee convergence.\n",
    "        \n",
    "        In ALS, in each epoch, using stochastic gradient descent, we find the optimal P matrix without changing Q.\n",
    "        Consequently, using the optimal P matrix hence obtained, we perform stochastic gradient descent for matrix Q.\n",
    "        This procedure repeats for each epoch, to yield the final optimal matrices P (user-feature), and Q (item-feature).\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    totalUsers = ratingsMatrix.shape[0]\n",
    "    totalMovies = ratingsMatrix.shape[1]\n",
    "    \n",
    "    P, Q, b_u, b_m, b = initialize_weights(trainSamples, K, totalUsers, totalMovies)\n",
    "    \n",
    "    cost = []\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)): \n",
    "        \n",
    "        for i in tqdm_notebook(range(len(trainSamples))):\n",
    "            \n",
    "            user, movie, ratingActual = trainSamples[i]\n",
    "            \n",
    "            ratingPredicted = get_rating(user, movie, b_u[user], b_m[movie], b, P[user,:], Q[movie,:])\n",
    "            error = ratingActual - ratingPredicted\n",
    "            \n",
    "            b_u[user] += alpha * (2 * error - lambda_reg * b_u[user])\n",
    "            b_m[movie] += alpha * (2 * error - lambda_reg * b_m[movie])\n",
    "            \n",
    "            P[user,:] += alpha * (2 * error * Q[movie,:] - lambda_reg * P[user,:])\n",
    "            \n",
    "        for i in tqdm_notebook(range(len(trainSamples))):\n",
    "            \n",
    "            user, movie, ratingActual = trainSamples[i]\n",
    "            \n",
    "            ratingPredicted = get_rating(user, movie, b_u[user], b_m[movie], b, P[user,:], Q[movie,:])\n",
    "            error = ratingActual - ratingPredicted\n",
    "            \n",
    "            b_u[user] += alpha * (2 * error - lambda_reg * b_u[user])\n",
    "            b_m[movie] += alpha * (2 * error - lambda_reg * b_m[movie])\n",
    "            \n",
    "            Q[movie,:] += alpha * (2 * error * P[user,:] - lambda_reg * Q[movie,:])\n",
    "            \n",
    "        _,_,RMSE,MAE = get_errors( predict_all_ratings(P, Q, b_u, b_m, b), trainSamples)\n",
    "\n",
    "        cost.append((RMSE,MAE))\n",
    "    \n",
    "    return P, Q, b_u, b_m, b, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent (alpha, epochs, trainSamples, K, ratingsMatrix, lambda_reg = 0):\n",
    "    '''\n",
    "        Performs stochastic gradient descent on the training samples, and returns the optimized user-feature\n",
    "        and item-feature matrices P and Q respectively, along with the optimized biases for users and items, \n",
    "        and the global bias.\n",
    "    '''\n",
    "    \n",
    "    totalUsers = ratingsMatrix.shape[0]\n",
    "    totalMovies = ratingsMatrix.shape[1]\n",
    "    \n",
    "    P, Q, b_u, b_m, b = initialize_weights(trainSamples, K, totalUsers, totalMovies)\n",
    "    \n",
    "    cost = []\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)): \n",
    "        \n",
    "        for i in tqdm_notebook(range(len(trainSamples))):\n",
    "            \n",
    "            user, movie, ratingActual = trainSamples[i]\n",
    "            \n",
    "            ratingPredicted = get_rating(user, movie, b_u[user], b_m[movie], b, P[user,:], Q[movie,:])\n",
    "            error = ratingActual - ratingPredicted\n",
    "\n",
    "            b_u[user] += alpha * (2 * error - lambda_reg * b_u[user])\n",
    "            b_m[movie] += alpha * (2 * error - lambda_reg * b_m[movie])\n",
    "            \n",
    "            P_i = P[user,:]\n",
    "            P[user,:] += alpha * (2 * error * Q[movie,:] - lambda_reg * P[user,:])\n",
    "            Q[movie,:] += alpha * (2 * error * P_i - lambda_reg * Q[movie,:])\n",
    "            \n",
    "        _,_,RMSE,MAE = get_errors( predict_all_ratings(P, Q, b_u, b_m, b), trainSamples)\n",
    "\n",
    "        cost.append((RMSE,MAE))\n",
    "    \n",
    "    return P, Q, b_u, b_m, b, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning : (Regularization Parameter *lambda* and Number of Factors *K*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_lambda (alpha, epochs, trainSamples, valSamples, K, ratingsMatrix, l, r):\n",
    "    '''\n",
    "        Finds the best regularization hyperparameter for the model, using Binary Search.\n",
    "        \n",
    "        The training using each candidate lambda value is done using the Training Samples,\n",
    "        and the evaluation of the performance of each such model is done using the Validation Samples.\n",
    "        \n",
    "        Based on the RMSE score on the Validation Set, the hyperparameter is adjusted so as to converge\n",
    "        to the minimum RMSE for the Validation Set. \n",
    "        \n",
    "        The model can then be evaluated on the Test Set.\n",
    "        \n",
    "    '''\n",
    "    lambdas = []\n",
    "    \n",
    "    for i in tqdm_notebook(range(5)):\n",
    "        \n",
    "        lambda_ = (l + r) / 2\n",
    "        \n",
    "        lambda_right = lambda_ + 0.01*lambda_\n",
    "        lambda_left = lambda_ - 0.01*lambda_\n",
    "        \n",
    "        P, Q, b_u, b_m, b,_ = stochastic_gradient_descent_ALS(alpha, epochs, trainSamples, K, ratingsMatrix, lambda_)\n",
    "        Pr, Qr, b_ur, b_mr, br,_ = stochastic_gradient_descent_ALS(alpha, epochs, trainSamples, K, ratingsMatrix, lambda_right)\n",
    "        Pl, Ql, b_ul, b_ml, bl,_ = stochastic_gradient_descent_ALS(alpha, epochs, trainSamples, K, ratingsMatrix, lambda_left)\n",
    "\n",
    "        _,_,RMSE,MAE = get_errors (predict_all_ratings(P, Q, b_u, b_m, b), valSamples)\n",
    "        _,_,RMSE_right,MAE_right = get_errors (predict_all_ratings(Pr, Qr, b_ur, b_mr, br), valSamples)\n",
    "        _,_,RMSE_left,MAE_left = get_errors (predict_all_ratings(Pl, Ql, b_ul, b_ml, bl), valSamples)\n",
    "        \n",
    "        \n",
    "        print(\"L_left = {}, L = {}, L_right = {}\".format(lambda_left, lambda_, lambda_right))\n",
    "        print(\"RMSE_left = {}, RMSE = {}, RMSE_right = {}\".format(RMSE_left, RMSE, RMSE_right))\n",
    "        lambdas.append(lambda_)\n",
    "        \n",
    "        if RMSE_right > RMSE and RMSE_left > RMSE:\n",
    "            \n",
    "            break\n",
    "            \n",
    "        elif RMSE_left < RMSE:\n",
    "            \n",
    "            r = lambda_\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            l = lambda_\n",
    "        \n",
    "    return lambda_, lambdas, RMSE, MAE, P, Q, b_u, b_m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters (possibleK, alpha, epochs, trainSamples, testSamples, valSamples, ratingsMatrix, l, r) :\n",
    "    '''\n",
    "        Tunes the hyperparamters K and lambda, by finding the best lambda for each possible K value,using\n",
    "        Binary Search.\n",
    "        The best combination of K and lambda, giving the least RMSE over the validation set can be chosen as the \n",
    "        optimal configuration for the model, and its performance evaluated on the test set.\n",
    "    '''\n",
    "    \n",
    "    lambdasK = []\n",
    "    bestLambdaK = []\n",
    "    performanceOnVal = []\n",
    "    performanceOnTest = []\n",
    "    \n",
    "    for K in possibleK :\n",
    "        \n",
    "        print(\"\\nK = \",K)\n",
    "        print(\"===========================================================\")\n",
    "        \n",
    "        P, Q, b_u, b_m, b,_ = stochastic_gradient_descent_ALS(alpha, epochs, trainSamples, K, ratingsMatrix)\n",
    "        _,_,RMSE,MAE = get_errors (predict_all_ratings(P, Q, b_u, b_m, b), valSamples)\n",
    "        \n",
    "        print(\"Without Regularization, (on Validation Set):\")\n",
    "        print(\"RMSE = {}, MAE = {} \\n\".format(RMSE, MAE))\n",
    "        print(\"Finding best regularization hyperparameter ...\")\n",
    "        \n",
    "        lambda_, lambdas, RMSE, MAE,P, Q, b_u, b_m, b = find_best_lambda(alpha, epochs, trainSamples, valSamples, K, ratingsMatrix, l, r)\n",
    "        \n",
    "        bestLambdaK.append(lambda_)\n",
    "        lambdasK.append(lambdas)\n",
    "        \n",
    "        print(\"After Regularization, (on Validation Set):\")\n",
    "        print(\"RMSE = {}, MAE = {} \\n\".format(RMSE, MAE))\n",
    "        print('Best lambda = ', lambda_)\n",
    "        performanceOnVal.append((RMSE,MAE))\n",
    "        \n",
    "        _,_,RMSE, MAE = get_errors (predict_all_ratings(P, Q, b_u, b_m, b), testSamples)\n",
    "        \n",
    "        print('\\nPerformance on Test Set:')\n",
    "        print(\"RMSE = {}, MAE = {} \\n\".format(RMSE, MAE))\n",
    "        \n",
    "        performanceOnTest.append((RMSE,MAE))\n",
    "        \n",
    "    return lambdasK, bestLambdaK, performanceOnVal, performanceOnTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_K (possibleK, alpha, epochs, trainSamples, testSamples, ratingsMatrix, lambda_reg = 0):\n",
    "    \n",
    "    '''\n",
    "        For a chosen lambda value, performs stochastic gradient descent for each candidate K value,\n",
    "        and returns the resulting performance of each K value for the given regularization parameter, on \n",
    "        the test set.\n",
    "    '''\n",
    "\n",
    "    testRMSE = []\n",
    "\n",
    "    for K in possibleK:\n",
    "        \n",
    "        print(\"K= \",K)\n",
    "        \n",
    "        P, Q, b_u, b_m, b, cost = stochastic_gradient_descent_ALS(alpha, epochs, trainSamples, K, ratingsMatrix, lambda_reg)\n",
    "        _,_,RMSE, MAE = get_errors (predict_all_ratings(P, Q, b_u, b_m, b), testSamples)\n",
    "        testRMSE.append((RMSE,MAE))\n",
    "    \n",
    "    return testRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # P, Q, b_u, b_m, b, cost500 = stochastic_gradient_descent_ALS(0.01, 20, trainSamples, 500, ratingsMatrix,0.01)\n",
    "    # P = np.load(\"P500.npy\")\n",
    "    # Q = np.load(\"Q500.npy\")\n",
    "    # b_m = np.load(\"b_m500.npy\")\n",
    "    # b_u = np.load(\"b_u500.npy\")\n",
    "    # b = np.load(\"b500.npy\")\n",
    "    \n",
    "    ratingsMatrix, trainSamples, testSamples, valSamples = preprocess()\n",
    "    \n",
    "    P, Q, b_u, b_m, b, cost = stochastic_gradient_descent_ALS(0.01, 20, trainSamples, 1000, ratingsMatrix, 0.01)\n",
    "    \n",
    "    allRatings = predict_all_ratings (P, Q, b_u, b_m, b)\n",
    "\n",
    "    _,_,RMSE,MAE = get_errors(allRatings, testSamples)\n",
    "\n",
    "    print(\"RMSE= \",RMSE)\n",
    "    print(\"MAE= \",MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.8793194552497463\n",
    " \n",
    " Test MAE = 0.6877353757628977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.8905157855499776\n",
    " \n",
    " Test MAE = 0.6929465135259187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.9391536000818118\n",
    " \n",
    " Test MAE = 0.7275229466603019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.9930651990946064\n",
    " \n",
    " Test MAE = 0.7699266990632744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 1.0102510579920232\n",
    " \n",
    " Test MAE = 0.7875524567174196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.9870828616500531\n",
    " \n",
    " Test MAE = 0.7722730412958273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.9568730897273094\n",
    " \n",
    " Test MAE = 0.7490492325871345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE = 0.9039870700310948\n",
    " \n",
    " Test MAE = 0.7045523325010874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0.01, K=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train RMSE = 0.14170220361792696\n",
    "\n",
    " Test RMSE = 0.8481153815446396\n",
    " \n",
    " Test MAE = 0.6665757266480518"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - ALS : 20 epochs, alpha = 0.01, lambda = 0, K=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train RMSE = 0.10471021346343064\n",
    "\n",
    " Test RMSE = 0.9109825227707479\n",
    " \n",
    " Test MAE = 0.7132322662513719"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
